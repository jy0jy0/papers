# Paper-List

- [Large Language Models](#large-language-models)
- [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
- [Prompt Engineering](#prompt-engineering)
- [Causal Recommender Systems](#causal-recommender-systems)
- [General Recommender Systems](#general-recommender-systems)
- [Causal Inference](#causal-inference)
- [NLP](#nlp)
- [ML Systems & Ops](#ml-systems--ops)
- [Time Series & Forecasting](#time-series--forecasting)


## Large Language Models

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2025/02 | [Do Large Language Models Reason Causally Like Us? Even Better?](https://arxiv.org/abs/2502.10215) | `llm` `causal reasoning` |  |  |
| 2024/02 | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | `survey` `llm` |  |  |
| 2023/10 | [A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4](https://arxiv.org/abs/2310.12321) | `survey` `gpt-3` `chatgpt` `gpt-4` |  |  |
| 2023/05 | [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://arxiv.org/abs/2305.08845) | `llm` `retrieval` `ranking` | [GitHub](https://github.com/RUCAIBox/LLMRank) |  |
| 2022/11 | [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) | `llm` `science` |  |  |
| 2021/09 | [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://paperswithcode.com/paper/a-study-on-hyperclova) | `llm` `korean` `scaling` |  | [Notion](https://spiny-passbook-5d6.notion.site/Intensive-Study-on-HyperCLOVA-Billions-scale-Korean-Generative-Pretrained-Transformers-c6cb64c9f563471485f51519c801a244?pvs=4) |
| 2017/06 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | `transformer` `attention` |  |  |

## Retrieval-Augmented Generation (RAG)

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2024/01 | [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059) | `rag` `abstractive summarization` | [GitHub](https://github.com/parthsarthi03/raptor) | [Blog](https://totravelhopefully.tistory.com/149) |
| 2023/12 | [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) | `survey` `rag` `llm` |  |  |
| 2022/12 | [Atlas: Few-Shot Learning with Retrieval-Augmented Models](https://arxiv.org/abs/2208.03299) | `rag` `few-shot` `retrieval` | [GitHub](https://github.com/facebookresearch/atlas) |  |
| 2020/05 | [RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) | `rag` `generation` `qa` |  |  |
| 2020/02 | [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) | `rag` `pretraining` `retrieval` | [GitHub](https://github.com/google-research/language/tree/master/language/realm) |  |

## Prompt Engineering

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2022/06 | [Pre-train Prompt Tuning with Human Feedback](https://arxiv.org/abs/2206.05826) | `prompt tuning` `rlhf` | [GitHub](https://github.com/eric-mitchell/pref) | |
| 2022/05 | [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) | `prompting` `chain-of-thought` |  | |
| 2022/04 | [Automatic Prompt Engineer (APE)](https://arxiv.org/abs/2211.01910) | `prompt generation` `search` | [GitHub](https://github.com/microsoft/LMOps/tree/main/llm/prompt) | |
| 2021/12 | [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190) | `prompt tuning` `generation` | [GitHub](https://github.com/XiangLi1999/PrefixTuning) | |
| 2021/10 | [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) | `prompt tuning` `efficient` | [GitHub](https://github.com/bojone/Prompt-Tuning) | |

## Causal Recommender Systems

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2023/07 | [Neural Causal Graph Collaborative Filtering](https://arxiv.org/abs/2307.04384) | `graph` `causal` |  |  |
| 2023/06 | [Path-Specific Counterfactual Fairness for Recommender Systems](https://arxiv.org/abs/2306.02615) | `counterfactual` `fairness` | [GitHub](https://github.com/yaochenzhu/PSF-VAE) |  |
| 2023/01 | [A Counterfactual Collaborative Session-based Recommender System](https://arxiv.org/abs/2301.13364) | `counterfactual` `session-based` |  |  |
| 2022/04 | [CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender System](https://arxiv.org/abs/2204.01266) | `counterfactual` `interactive` | [GitHub](https://github.com/chongminggao/CIRS-codes) |  |
| 2022/04 | [Causal Representation Learning for Out-of-Distribution Recommendation](https://hexiangnan.github.io/papers/www22-ood-rec.pdf) | `ood` `causal` | [GitHub](https://github.com/Linxyhaha/COR) ||
| 2021/10 | [Deconfounded Causal Collaborative Filtering](https://arxiv.org/abs/2110.07122) | `causal` `collaborative filtering` | [GitHub](https://github.com/rutgerswiselab/DCCF) |  |
| 2021/07 | [Counterfactual Data-Augmented Sequential Recommendation](https://yongfeng.me/attach/wang-sigir2021.pdf) | `counterfactual` `sequential` |  |  |
| 2020/10 | [CAFE: Causal Adaptive Feature Enhancement for Sequential Recommendation](https://arxiv.org/abs/2010.15620) | `causal` `feature learning` | [GitHub](https://github.com/JiachengLi1995/CAFE) |  |

## General Recommender Systems

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2022/02 | [CLSR: Disentangling Long and Short-Term Interests for Recommendation](https://arxiv.org/abs/2202.13090) | `user modeling` `sequential` | [GitHub](https://github.com/tsinghua-fib-lab/CLSR) | [Blog](https://totravelhopefully.tistory.com/156) |
| 2020/06 | [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) | `collaborative filtering` `deep learning` | [GitHub](https://github.com/hexiangnan/neural_collaborative_filtering) |  |
| 2017/07 | [Deep Learning based Recommender System: A Survey and New Perspectives](https://arxiv.org/abs/1707.07435) | `survey` `deep learning` |  |  |
| 2016/10 | [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf) | `youtube` `large-scale` | [GitHub](https://github.com/hyez/Deep-Youtube-Recommendations) | [Blog](https://totravelhopefully.tistory.com/121) |
| 2009/03 | [Matrix Factorization Techniques for Recommender Systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf) | `matrix factorization` `collaborative filtering` |  |  |
| 2003/02 | [Amazon.com Recommendations: Item-to-Item Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf) | `collaborative filtering` `item-to-item` | [GitHub](https://github.com/oni-on/item-collaborative-filtering) | [Blog](https://totravelhopefully.tistory.com/139) |

## Causal Inference

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2024/02 | [Robust Agents Learn Causal World Models](https://arxiv.org/abs/2402.10877) | `causal modeling` `rl` |  |  |
| 2023/10 | [Causal Inference as a Blind Spot of Data Scientists](https://www.dzidas.com/ml/2023/10/15/blind-spot-ds/) | `opinion` `causality` |  |  |
| 2023/08 | [Causally Estimating the Effect of YouTube's Recommender System using Counterfactual Bots](https://arxiv.org/abs/2308.10398) | `evaluation` `youtube` `counterfactual` |  |  |
| 2022/03 | [Differentiable DAG Sampling](https://arxiv.org/abs/2203.08509) | `causal discovery` `dag` |  |  |
| 2016/08 | [Double/Debiased Machine Learning for Treatment and Causal Parameters](https://arxiv.org/abs/1608.00060) | `econometrics` `treatment effect` | [GitHub](https://github.com/Microsoft/EconML) |  |

## NLP

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2022/03 | [Language Modeling via Stochastic Processes](https://github.com/rosewang2008/language_modeling_via_stochastic_processes) | `probabilistic` `language modeling` | [GitHub](https://github.com/rosewang2008/language_modeling_via_stochastic_processes) | [Blog](https://totravelhopefully.tistory.com/89) |
| 2021/09 | [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) | `entity linking` `ner` `self-attention` | [GitHub](https://github.com/studio-ousia/luke) ||
| 2020/10 | [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) | `text-to-text` `pretraining` `transfer learning` | [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) | |
| 2020/03 | [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB) | `pretraining` `text encoders` | [GitHub](https://github.com/google-research/electra) | [Notion](https://spiny-passbook-5d6.notion.site/ELECTRA-PRE-TRAINING-TEXT-ENCODERS-AS-DISCRIMINATORS-RATHER-THAN-GENERATORS-9e953f5d898f4af4a054f7e4306f7694) |
| 2019/10 | [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) | `pretraining` `autoregressive` | [GitHub](https://github.com/zihangdai/xlnet) | |
| 2018/06 | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | `pretraining` `masked lm` `bert` | [GitHub](https://github.com/google-research/bert) | |


## ML Systems & Ops

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2024/12 | [Good Practices for Evaluation of Machine Learning Systems](https://arxiv.org/abs/2412.03700) | `ml evaluation`|  |  |
| 2023/10 | [Test & Evaluation Best Practices for Machine Learning-Enabled Systems](https://arxiv.org/abs/2310.06800) | `ml testing` `best practices` |  |  |
| 2022/05 | [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/pdf/2205.02302) | `mlops` `architecture` |  |  |
| 2018/01 | [Evaluation of Interactive Machine Learning Systems](https://arxiv.org/abs/1801.07964) | `interactive ml` `evaluation` |  |  |
| 2015 | [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) | `mlops` `engineering` `debt` |  |  |

## Time Series / Forecasting

| Published | Title | Tags | Code | Note |
|-----------|-------|------|------|------|
| 2023/10 | [TimeGPT: Foundation Models for Time Series Forecasting](https://arxiv.org/abs/2310.02255) | `foundation model` `time series` `gpt` | [GitHub](https://github.com/nixtla/timegpt) | |
| 2022/05 | [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) | `transformer` `forecasting` |  | [Blog](https://totravelhopefully.tistory.com/56) |
| 2021/06 | [TS2Vec: Unsupervised Representation Learning for Time Series](https://arxiv.org/abs/2106.10466) | `unsupervised` `representation` | [GitHub](https://github.com/yuezhihan/ts2vec) | |
| 2020/12 | [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) | `transformer` `long sequence` | [GitHub](https://github.com/zhouhaoyi/Informer2020) | [Notion](https://spiny-passbook-5d6.notion.site/Are-Transformers-Effective-for-Time-Series-Forecasting-12811fbc6dd446a185f1c0306666b693) |
